{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom datasets import load_dataset\nfrom datasets import concatenate_datasets\nfrom collections import Counter\n\n#loading the dataset\nds = load_dataset(\"wellCh4n/tomato-leaf-disease-image\")\n\n\noriginal_labels = sorted(list(set(ds[\"train\"][\"label\"])))\nlabel_mapping = {old: new for new, old in enumerate(original_labels)}\n\ndef remap_label(example):\n    example[\"label\"] = label_mapping[example[\"label\"]]\n    return example\n\nds = ds.map(remap_label)\n\n#Combining the 2 splits into one dataset\nds_full = concatenate_datasets([ds[\"train\"], ds[\"validation\"]])\n\n#Data split into 70/15/15\ntrain_test = ds_full.train_test_split(test_size=0.3, stratify_by_column=\"label\", seed=0)\ndev_test = train_test[\"test\"].train_test_split(test_size=0.5,stratify_by_column=\"label\", seed=0)\n\nds_split = {\n    \"train\": train_test[\"train\"],\n    \"validation\": dev_test[\"train\"],\n    \"test\": dev_test[\"test\"]\n}\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-22T17:05:17.842638Z","iopub.execute_input":"2025-11-22T17:05:17.842975Z","iopub.status.idle":"2025-11-22T17:05:36.069360Z","shell.execute_reply.started":"2025-11-22T17:05:17.842951Z","shell.execute_reply":"2025-11-22T17:05:36.068784Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2900510bd5674b34881c4526c302a1f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/224M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cba60cefcedd44d3b6993015ec394e97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/56.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4257d7d839a0472daa16bb0cfa09e203"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14218 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52693e950bab40fcac30b75d5fb95f78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3569 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6e6d824eab9428784b8919cd434efa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14218 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f22f183f9754a95a81793c4a101d67b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3569 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"926b2e58a180442ab3900269352d0e5f"}},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"<h3>Displaying the Class Distribution</h3>","metadata":{}},{"cell_type":"code","source":"def print_distribution(name, dataset):\n    labels = dataset[\"label\"]\n    counts = Counter(labels)\n    print(f\"\\n{name} set distribution:\")\n    for label, count in sorted(counts.items()):\n        print(f\"  Class {label}: {count} samples ({count/len(labels) * 100:.2f}%)\")\n    print(f\"  Total: {len(labels)} samples\")\n\nprint_distribution(\"Train\", ds_split[\"train\"])\nprint_distribution(\"Validation\", ds_split[\"validation\"])\nprint_distribution(\"Test\", ds_split[\"test\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T17:10:08.765352Z","iopub.execute_input":"2025-11-22T17:10:08.765893Z","iopub.status.idle":"2025-11-22T17:10:09.598007Z","shell.execute_reply.started":"2025-11-22T17:10:08.765866Z","shell.execute_reply":"2025-11-22T17:10:09.597358Z"}},"outputs":[{"name":"stdout","text":"\nTrain set distribution:\n  Class 0: 1114 samples (8.95%)\n  Class 1: 666 samples (5.35%)\n  Class 2: 983 samples (7.90%)\n  Class 3: 1336 samples (10.73%)\n  Class 4: 700 samples (5.62%)\n  Class 5: 1489 samples (11.96%)\n  Class 6: 1239 samples (9.95%)\n  Class 7: 3750 samples (30.12%)\n  Class 8: 1173 samples (9.42%)\n  Total: 12450 samples\n\nValidation set distribution:\n  Class 0: 239 samples (8.96%)\n  Class 1: 143 samples (5.36%)\n  Class 2: 211 samples (7.91%)\n  Class 3: 286 samples (10.72%)\n  Class 4: 150 samples (5.62%)\n  Class 5: 319 samples (11.96%)\n  Class 6: 266 samples (9.97%)\n  Class 7: 803 samples (30.10%)\n  Class 8: 251 samples (9.41%)\n  Total: 2668 samples\n\nTest set distribution:\n  Class 0: 238 samples (8.92%)\n  Class 1: 143 samples (5.36%)\n  Class 2: 210 samples (7.87%)\n  Class 3: 287 samples (10.75%)\n  Class 4: 150 samples (5.62%)\n  Class 5: 319 samples (11.95%)\n  Class 6: 266 samples (9.97%)\n  Class 7: 804 samples (30.12%)\n  Class 8: 252 samples (9.44%)\n  Total: 2669 samples\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"<h3>Extracting features for Machine Learning</h3>","metadata":{}},{"cell_type":"code","source":"import cv2\nfrom skimage.feature import hog, local_binary_pattern\nfrom skimage.color import rgb2gray\nfrom skimage.transform import resize\n\nRESIZE_SHAPE = (128, 128)\n\n# HOG parameters\nHOG_ORIENTATIONS = 9\nHOG_PIXELS_PER_CELL = (8, 8)\nHOG_CELLS_PER_BLOCK = (2, 2)\n\n# LBP parameters\nLBP_RADIUS = 3\nLBP_POINTS = 8 * LBP_RADIUS\n\ndef extract_features(image):\n\n    img_resized = resize(image, RESIZE_SHAPE, anti_aliasing=True)\n    img_resized = (img_resized * 255).astype(np.uint8)\n\n   #HOG\n    gray_float = rgb2gray(img_resized)\n    gray = (gray_float * 255).astype(np.uint8) \n    \n    hog_features = hog(\n        gray,\n        orientations=HOG_ORIENTATIONS,\n        pixels_per_cell=HOG_PIXELS_PER_CELL,\n        cells_per_block=HOG_CELLS_PER_BLOCK,\n        visualize=False,\n    )\n\n    #LBP\n    lbp = local_binary_pattern(gray, LBP_POINTS, LBP_RADIUS, method=\"uniform\")\n\n    lbp_hist, _ = np.histogram(\n        lbp.ravel(),\n        bins=np.arange(0, LBP_POINTS + 3),\n        range=(0, LBP_POINTS + 2)\n    )\n    lbp_hist = lbp_hist.astype(\"float\")\n    lbp_hist /= (lbp_hist.sum() + 1e-7) \n\n    #Color Histogram\n    hsv = cv2.cvtColor(img_resized, cv2.COLOR_RGB2HSV)\n\n    hsv_hist = []\n    hist_sizes = [180, 256, 256]\n    ranges = [(0, 180), (0, 256), (0, 256)]\n\n    for ch, (bins, rng) in enumerate(zip(hist_sizes, ranges)):\n        hist = cv2.calcHist([hsv], [ch], None, [bins], rng)\n        hsv_hist.extend(hist.flatten())\n\n    hsv_hist = np.array(hsv_hist, dtype=np.float32)\n\n    hsv_hist /= (np.sum(hsv_hist) + 1e-7)\n\n    final_vector = np.concatenate([\n        hog_features,\n        lbp_hist,\n        hsv_hist\n    ])\n\n    return final_vector","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T17:10:12.038885Z","iopub.execute_input":"2025-11-22T17:10:12.039589Z","iopub.status.idle":"2025-11-22T17:10:12.418957Z","shell.execute_reply.started":"2025-11-22T17:10:12.039565Z","shell.execute_reply":"2025-11-22T17:10:12.418446Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef process_split(dataset_split):\n    X = []\n    y = []\n\n    for item in tqdm(dataset_split, desc=\"Extracting features\", ncols=100):\n        img = np.array(item[\"image\"].convert(\"RGB\"))\n\n        features = extract_features(img)\n\n        X.append(features)\n        y.append(item[\"label\"])\n\n    X = np.array(X, dtype=np.float32)\n    y = np.array(y, dtype=np.int64)\n\n    return X, y\n\n\nX_train, y_train = process_split(ds_split[\"train\"])\nprint(\"Train features shape:\", X_train.shape)\n\nX_validation, y_validation = process_split(ds_split[\"validation\"])\nprint(\"Dev features shape:\", X_validation.shape)\n\nX_test, y_test = process_split(ds_split[\"test\"])\nprint(\"Test features shape:\", X_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T17:10:14.372760Z","iopub.execute_input":"2025-11-22T17:10:14.373658Z","iopub.status.idle":"2025-11-22T17:16:03.846932Z","shell.execute_reply.started":"2025-11-22T17:10:14.373628Z","shell.execute_reply":"2025-11-22T17:16:03.846119Z"}},"outputs":[{"name":"stderr","text":"Extracting features: 100%|████████████████████████████████████| 12450/12450 [04:05<00:00, 50.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train features shape: (12450, 8818)\n","output_type":"stream"},{"name":"stderr","text":"Extracting features: 100%|██████████████████████████████████████| 2668/2668 [00:51<00:00, 51.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dev features shape: (2668, 8818)\n","output_type":"stream"},{"name":"stderr","text":"Extracting features: 100%|██████████████████████████████████████| 2669/2669 [00:51<00:00, 51.33it/s]","output_type":"stream"},{"name":"stdout","text":"Test features shape: (2669, 8818)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_validation_scaled = scaler.transform(X_validation)\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T17:16:03.848321Z","iopub.execute_input":"2025-11-22T17:16:03.848724Z","iopub.status.idle":"2025-11-22T17:16:06.128502Z","shell.execute_reply.started":"2025-11-22T17:16:03.848697Z","shell.execute_reply":"2025-11-22T17:16:06.127703Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"<h3>Training Machine Learning Models</h3>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import ParameterGrid\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T17:16:06.129348Z","iopub.execute_input":"2025-11-22T17:16:06.129781Z","iopub.status.idle":"2025-11-22T17:16:06.651959Z","shell.execute_reply.started":"2025-11-22T17:16:06.129745Z","shell.execute_reply":"2025-11-22T17:16:06.651158Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nfinal_model = XGBClassifier(\n    objective=\"multi:softmax\",\n    num_class=4,\n    tree_method=\"hist\",   # fast and stable\n    learning_rate=0.05,\n    n_estimators=400,\n    max_depth=6,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    gamma=0.1,\n    min_child_weight=3,\n    eval_metric=\"mlogloss\"\n)\n\nfinal_model.fit(X_train_scaled, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T17:16:06.653482Z","iopub.execute_input":"2025-11-22T17:16:06.654190Z","iopub.status.idle":"2025-11-22T18:08:09.433831Z","shell.execute_reply.started":"2025-11-22T17:16:06.654169Z","shell.execute_reply":"2025-11-22T18:08:09.432823Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/529240358.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1517\u001b[0m             )\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1520\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             _check_call(\n\u001b[0;32m-> 2051\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"<h4>Random Forest</h4>","metadata":{}},{"cell_type":"code","source":"def evaluate(model, Xv, yv):\n    preds = model.predict(Xv)\n    return accuracy_score(yv, preds)\n\nrf_param_grid = {\n    \"n_estimators\": [50, 100],\n    \"max_depth\": [10, 20],\n    \"min_samples_split\": [2, 5],\n}\n\nbest_rf_acc = -1\nbest_rf_params = None\n\nfor params in tqdm(ParameterGrid(rf_param_grid)):\n    rf = RandomForestClassifier(\n        class_weight='balanced',\n        random_state=0,\n        **params\n    )\n    rf.fit(X_train_scaled, y_train)\n    acc = evaluate(rf, X_validation_scaled, y_validation)\n\n    if acc > best_rf_acc:\n        best_rf_acc = acc\n        best_rf_params = params\n\nprint(\"Best RF Params:\", best_rf_params)\nprint(\"Validation Accuracy:\", best_rf_acc)\n\n#Retraining\nrf_final = RandomForestClassifier(\n    class_weight='balanced',\n    random_state=34,\n    **best_rf_params\n)\nrf_final.fit(\n    np.concatenate([X_train_scaled, X_validation_scaled]),\n    np.concatenate([y_train, y_validation])\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T08:25:54.428871Z","iopub.execute_input":"2025-11-21T08:25:54.429260Z","iopub.status.idle":"2025-11-21T08:37:41.529863Z","shell.execute_reply.started":"2025-11-21T08:25:54.429242Z","shell.execute_reply":"2025-11-21T08:37:41.529203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4>SVM</h4>","metadata":{}},{"cell_type":"code","source":"\nsvm_final = SVC(\n    class_weight=\"balanced\",\n    random_state=0,\n    C=10,\n    gamma=\"scale\",\n    kernel=\"rbf\"\n)\n\nsvm_final.fit(\n    np.concatenate([X_train_scaled, X_validation_scaled]),\n    np.concatenate([y_train, y_validation])\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T08:37:41.530574Z","iopub.execute_input":"2025-11-21T08:37:41.530808Z","iopub.status.idle":"2025-11-21T08:57:58.843131Z","shell.execute_reply.started":"2025-11-21T08:37:41.530778Z","shell.execute_reply":"2025-11-21T08:57:58.842333Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h4>K-Nearest Neighbor (KNN)</h4>","metadata":{}},{"cell_type":"code","source":"knn_param_grid = {\n    \"n_neighbors\": [3, 5, 7, 9],\n    \"weights\": [\"uniform\", \"distance\"],\n    \"p\": [1, 2],\n}\n\nbest_knn_acc = -1\nbest_knn_params = None\n\nfor params in tqdm(ParameterGrid(knn_param_grid)):\n    knn = KNeighborsClassifier(**params)\n    knn.fit(X_train_scaled, y_train)\n    acc = evaluate(knn, X_validation_scaled, y_validation)\n\n    if acc > best_knn_acc:\n        best_knn_acc = acc\n        best_knn_params = params\n\nprint(\"Best KNN Params:\", best_knn_params)\nprint(\"Validation Accuracy:\", best_knn_acc)\n\n# Retraining\nknn_final = KNeighborsClassifier(**best_knn_params)\nknn_final.fit(\n    np.concatenate([X_train_scaled, X_validation_scaled]),\n    np.concatenate([y_train, y_validation])\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T08:57:58.843994Z","iopub.execute_input":"2025-11-21T08:57:58.844270Z","iopub.status.idle":"2025-11-21T09:28:26.919500Z","shell.execute_reply.started":"2025-11-21T08:57:58.844239Z","shell.execute_reply":"2025-11-21T09:28:26.918825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h3>Testing the Models</h3>","metadata":{}},{"cell_type":"code","source":"def final_results(model, Xtest, ytest, name):\n    preds = model.predict(Xtest)\n    print(f\"{name}:\")\n    print(f\"Accuracy: {(accuracy_score(ytest, preds) * 100):.2f}%\")\n    print(\"\\nClassification Report:\\n\", classification_report(ytest, preds))\n    cm = confusion_matrix(ytest, preds)\n\n    # Plot Confusion Matrix\n    plt.figure(figsize=(7,6))\n    sns.heatmap(cm, annot=False, cmap=\"Blues\")\n    plt.title(f\"{name} Confusion Matrix\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T09:28:26.920339Z","iopub.execute_input":"2025-11-21T09:28:26.920852Z","iopub.status.idle":"2025-11-21T09:28:26.925283Z","shell.execute_reply.started":"2025-11-21T09:28:26.920831Z","shell.execute_reply":"2025-11-21T09:28:26.924734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_results(final_model,  x_test, y_test, \"XGboost\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_results(rf_final,  X_test_scaled, y_test, \"Random Forest\")\nfinal_results(svm_final, X_test_scaled, y_test, \"SVM\")\nfinal_results(knn_final, X_test_scaled, y_test, \"KNN\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T09:28:26.925850Z","iopub.execute_input":"2025-11-21T09:28:26.926026Z","iopub.status.idle":"2025-11-21T09:37:52.394944Z","shell.execute_reply.started":"2025-11-21T09:28:26.926012Z","shell.execute_reply":"2025-11-21T09:37:52.394212Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h3>Processing Images for Deep Learning</h3>","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ndef hf_to_tfds(hf_split):\n    def gen():\n        for item in hf_split:\n            img = item[\"image\"]\n            img = tf.keras.preprocessing.image.img_to_array(img)\n            label = item[\"label\"]\n            yield img, label\n\n    return tf.data.Dataset.from_generator(\n        gen,\n        output_types=(tf.float32, tf.int32),\n        output_shapes=((None, None, 3), ())\n    )\n\ntrain_ds_raw = hf_to_tfds(ds_split[\"train\"])\nval_ds_raw   = hf_to_tfds(ds_split[\"validation\"])\ntest_ds_raw  = hf_to_tfds(ds_split[\"test\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T09:37:52.395762Z","iopub.execute_input":"2025-11-21T09:37:52.396064Z","iopub.status.idle":"2025-11-21T09:38:08.268529Z","shell.execute_reply.started":"2025-11-21T09:37:52.396043Z","shell.execute_reply":"2025-11-21T09:38:08.267806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE = 128\n\n# Resize images and prepare for resnet\ndef preprocess(image, label):\n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    image = image / 255\n    return image, label\n\n# Augmentation\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(0.15),\n    tf.keras.layers.RandomZoom(0.1),\n    tf.keras.layers.RandomContrast(0.15),\n], name=\"data_augmentation\")\n\ndef augment(image, label):\n    return data_augmentation(image), label\n\nBATCH_SIZE = 32\n\n# Train dataset\ntrain_ds = (\n    train_ds_raw\n    .shuffle(2000)\n    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n    .map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.AUTOTUNE)\n)\n\n# Validation dataset\nval_ds = (\n    val_ds_raw\n    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.AUTOTUNE)\n)\n\n# Test dataset\ntest_ds = (\n    test_ds_raw\n    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.AUTOTUNE)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T09:38:08.269252Z","iopub.execute_input":"2025-11-21T09:38:08.269884Z","iopub.status.idle":"2025-11-21T09:38:08.850446Z","shell.execute_reply.started":"2025-11-21T09:38:08.269866Z","shell.execute_reply":"2025-11-21T09:38:08.849664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculating the class weights\ndef compute_class_weights(labels):\n    counts = Counter(labels)\n    total = sum(counts.values())\n\n    class_weights = {\n        cls: total / (len(counts) * count)\n        for cls, count in counts.items()\n    }\n    return class_weights\n\nlabels_list = [item[\"label\"] for item in ds_split[\"train\"]]\nclass_weights = compute_class_weights(labels_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T09:38:08.851277Z","iopub.execute_input":"2025-11-21T09:38:08.852148Z","iopub.status.idle":"2025-11-21T09:38:17.317672Z","shell.execute_reply.started":"2025-11-21T09:38:08.852123Z","shell.execute_reply":"2025-11-21T09:38:17.317063Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h3>Building and training the CNN</h3>","metadata":{}},{"cell_type":"code","source":"def create_cnn_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=9):\n    inputs = tf.keras.Input(shape=input_shape)\n\n    x = tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n\n    x = tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n\n    x = tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n\n    x = tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\nmodel = create_cnn_model(input_shape=(IMG_SIZE, IMG_SIZE, 3))\nmodel.summary()\n\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[\"accuracy\"]\n)\n\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=5,\n    restore_best_weights=True\n)\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    \"best_epoch.h5\",\n    monitor=\"val_loss\",\n    save_best_only=True,\n    verbose=1\n)\n\n\nEPOCHS = 50\n\nhistory = model.fit(\n    train_ds,\n    epochs=EPOCHS,\n    validation_data=val_ds,\n    class_weight=class_weights,\n    callbacks=[early_stop, checkpoint]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T09:38:17.318392Z","iopub.execute_input":"2025-11-21T09:38:17.318658Z","iopub.status.idle":"2025-11-21T09:57:23.537939Z","shell.execute_reply.started":"2025-11-21T09:38:17.318636Z","shell.execute_reply":"2025-11-21T09:57:23.537358Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h3>Testing the CNN</h3>","metadata":{}},{"cell_type":"code","source":"\ny_true = []\ny_pred = []\n\nfor images, labels in test_ds:\n    preds = model.predict(images)\n    preds = np.argmax(preds, axis=1)\n\n    y_true.extend(labels.numpy())\n    y_pred.extend(preds)\n\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n\nacc = accuracy_score(y_true, y_pred)\nprint(f\"Test Accuracy: {acc * 100:.2f}%\")\n\nprint(\"\\nCNN Classification Report:\")\nprint(classification_report(y_true, y_pred))\n\ncm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=False, cmap='Blues')\nplt.title(\"CNN Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T09:57:23.539283Z","iopub.execute_input":"2025-11-21T09:57:23.539541Z","iopub.status.idle":"2025-11-21T09:57:34.564934Z","shell.execute_reply.started":"2025-11-21T09:57:23.539523Z","shell.execute_reply":"2025-11-21T09:57:34.564375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}